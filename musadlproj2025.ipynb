{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12090571,"sourceType":"datasetVersion","datasetId":5857}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In questo progetto in preparazione all'esame di Deep learning ho pensato di affrontare un problema di classificazione.\nIn questo progetto, sfruttando un dataset disponibile online riguardante i frutti, esploreremo i dati a disposizione, addestreremo un sistema alla classificazione dei frutti e etesteremo altre funzionalità come la creazione di immagini di frutti.","metadata":{"id":"psBMRpDC5C6L"}},{"cell_type":"markdown","source":"Montiamo Google Drive per usarlo con il notebook per contenere codice e dati.","metadata":{"id":"EuWyHXoN8v2c"}},{"cell_type":"code","source":"# SOLO SU GOOGLE COLAB\nfrom google.colab import drive\ndrive.mount('/content/drive')","metadata":{"id":"5wG9IOyI6MD7","outputId":"651c6505-274b-40d1-9671-ebbec5e281c8","trusted":true,"execution":{"iopub.status.busy":"2025-06-29T20:47:38.995774Z","iopub.execute_input":"2025-06-29T20:47:38.995965Z","iopub.status.idle":"2025-06-29T20:47:39.094226Z","shell.execute_reply.started":"2025-06-29T20:47:38.995945Z","shell.execute_reply":"2025-06-29T20:47:39.092882Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Ottenere il dataset per l'addestramento**\nScarichiamo il dataset per l'addestramento sfruttando da kaggle e lo decomprimiamo sfruttando la possibilità offerta da Google Colab di inviare istruzioni shell bash","metadata":{"id":"ISkQA-D582CI"}},{"cell_type":"code","source":"# SOLO SU GOOGLE COLAB\n!mkdir -p /content/drive/MyDrive/FruitDLProj # Crea il path qualora non esista","metadata":{"id":"gAcE23S2i32w"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# SOLO SU GOOGLE COLAB\n!curl -L -o /content/drive/MyDrive/FruitDLProj/fruits.zip\\\n  https://www.kaggle.com/api/v1/datasets/download/moltean/fruits","metadata":{"id":"mntA_BF78itz","outputId":"7ea8f2a5-e4c8-4644-8724-9f8a76e2f9e7"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# SOLO SU GOOGLE COLAB\n!unzip /content/drive/MyDrive/FruitDLProj/fruits.zip -d /content/drive/MyDrive/FruitDLProj/frutti","metadata":{"collapsed":true,"id":"N8WjswcY-gl8","outputId":"7e71323d-c2c9-40bd-ad05-ab108989f309","jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# SOLO SU GOOGLE COLAB\n!ls /content/drive/MyDrive/FruitDLProj/frutti","metadata":{"id":"r6YXh5Hw9U_8","outputId":"928eb11b-9004-4ea9-bb68-c71e84807ce9"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Esplorazione dei dati**\nIniziamo l'esplorazione del dataset:\n*   valuteremo la sua divisione in train e Test ricavando il numero di classi e le etichette associate dalla struttura dei file.\n*   Valuteremo la suddivisione delle entry per ogni classe per Train e test\n*   Rappresenteremo con un grafico a barre questa suddivisione\n*   Vedremo un raffronto Train/Test per le 5 classi più popolate\n*   Costruiremo una nuvola di punti di un campione di dati per apprezzare visivamente la distribuzione delle entry in gruppi distinti, nel fare questo dovremo ridurre la dimensionalità dei dati al fine della visualizzazione grafica.\n\n\n\n\n\n","metadata":{"id":"_GAHoVpxOs3D"}},{"cell_type":"code","source":"import os # Libreria per interagire con l'OS\nimport matplotlib.pyplot as plt # Libreria per la generazione di grafici\nimport seaborn as sns # Libreria per dei grafici visivamente più efficaci\nimport pandas as pd # Libreria per la manipolazione e l'analisi dei dati tabellari\nimport matplotlib.image as mpimg # Modulo specifico di Matplotlib per caricare e manipolare immagini\nimport numpy as np # Libreria fondamentale per il calcolo numerico con funzioni quali la manipolazione dei tensori\nfrom collections import Counter # Libreria utile per contare gli elementi in una collezione\nfrom PIL import Image # Libreria per caricare e manipolare le immagini\nfrom sklearn.manifold import TSNE # Libreria per la riduzione di dimensionalità\nfrom sklearn.preprocessing import StandardScaler # Libreria per scalare i dati prima di t-SNE\nimport random # Libreria che fornisce funzionalità per generare numeri casuali e per operare con elementi casuali\nimport time # Libreria per misurare il tempo di esecuzione di diverse parti del tuo codice utile per monitorare le prestazion\n\n# 1. Definizione dei percorsi del dataset\n# path su Google Colab\n#dataset_root = '/content/drive/MyDrive/FruitDLProj/frutti/fruits-360_100x100/fruits-360'\ndataset_root = '/kaggle/input/fruits/fruits-360_100x100/fruits-360'\n\n# Suddivisione dei percorsi con i dati di train e di test\ntrain_dir = os.path.join(dataset_root, 'Training')\ntest_dir = os.path.join(dataset_root, 'Test')\n\n# 2. Preparazione dei dati per i grafici\n# Otteniamo le classi dai nomi delle cartelle\n\n# Ordiniamo le classi (tipi di frutta)\ntrain_classes = sorted(os.listdir(train_dir))\ntest_classes = sorted(os.listdir(test_dir))\n\n# Contiamo il numero di classi\nnum_train_classes = len(train_classes)\nnum_test_classes = len(test_classes)\n\nprint(f\"Numero di classi nel Training set: {num_train_classes}\")\nprint(f\"Numero di classi nel Test set: {num_test_classes}\")\nprint(f\"Le classi nel Training set sono: {train_classes[:5]} ... (mostro solo le prime 5)\") # Esempio delle prime 5 classi\nprint(\"-\" * 30)\n\n# Verifichiamo se le classi sono le stesse nei due set\nif train_classes == test_classes:\n    print(\"Le classi nel training e nel test set corrispondono.\")\n    all_classes = train_classes\nelse:\n    print(\"Attenzione: Le classi nel training e nel test set NON corrispondono completamente.\")\n    all_classes = sorted(list(set(train_classes + test_classes))) # Unisce ed ordina le classi uniche in una lista\n\nif not all_classes:\n    print(f\"Errore: Nessuna classe trovata nella directory di training e test: {train_dir}, {test_dir}\")\n    print(\"Controlla il percorso del dataset.\")\nelse:\n    print(f\"Trovate {len(all_classes)} classi di frutta.\")\n\n\n# Contiamo le immagini per classe nel Training Set\ntrain_image_counts = {}\nfor fruit_class in all_classes:\n    class_path = os.path.join(train_dir, fruit_class)\n    if os.path.exists(class_path) and os.path.isdir(class_path):\n        train_image_counts[fruit_class] = len(os.listdir(class_path))\n    else:\n        train_image_counts[fruit_class] = 0 # In caso la classe non esista o non sia una directory\n\n# Contiamo le immagini per classe nel Test Set\ntest_image_counts = {}\nfor fruit_class in all_classes:\n    class_path = os.path.join(test_dir, fruit_class)\n    if os.path.exists(class_path) and os.path.isdir(class_path):\n        test_image_counts[fruit_class] = len(os.listdir(class_path))\n    else:\n        test_image_counts[fruit_class] = 0 # In caso la classe non esista o non sia una directory\n\n# Converto i dizionari in DataFrame per facilitare la creazione dei grafici\ndf_train = pd.DataFrame(train_image_counts.items(), columns=['Classe', 'Numero Immagini'])\ndf_test = pd.DataFrame(test_image_counts.items(), columns=['Classe', 'Numero Immagini'])\n\n# Ordino per numero di immagini per una migliore visualizzazione\ndf_train = df_train.sort_values(by='Numero Immagini', ascending=False)\ndf_test = df_test.sort_values(by='Numero Immagini', ascending=False)\n","metadata":{"id":"GuDtct-TR7ix","outputId":"35c0f624-5850-497c-e69e-b138b793fe38","trusted":true,"execution":{"iopub.status.busy":"2025-07-01T16:03:34.109912Z","iopub.execute_input":"2025-07-01T16:03:34.110234Z","iopub.status.idle":"2025-07-01T16:03:41.379316Z","shell.execute_reply.started":"2025-07-01T16:03:34.110209Z","shell.execute_reply":"2025-07-01T16:03:41.378772Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Suddivisione delle entry per classe nei Train Set e Test Set**","metadata":{"id":"ib1far8_-uqo"}},{"cell_type":"code","source":"# 3. Creazione dei grafici a barre (Barplot)\n\n# Seleziona solo le prime 15 classi per una miglior visualizzazione\ndf_train_top = df_train.head(15)\ndf_test_top = df_test.head(15)\n\nplt.style.use('seaborn-v0_8-darkgrid') # Applichiamo lo stile al grafico usando seaborn\n\n# Grafico per il Training Set\nplt.figure(figsize=(25, 9)) # Aumentiamo la dimensione del grafico per leggibilità\nsns.barplot(x='Classe', y='Numero Immagini', data=df_train_top, palette='viridis', hue='Classe')\nplt.title('Distribuzione Immagini per Classe nel Training Set', fontsize=28) # Titolo del grafico\nplt.xlabel('Classe di Frutta', fontsize=22) # Etichetta dell'asse X\nplt.ylabel('Numero di Immagini', fontsize=22) # Etichetta dell'asse Y\nplt.xticks(rotation=90, fontsize=18) # Ruota le etichette dell'asse X per evitare sovrapposizioni\nplt.tight_layout() # Adatta il layout per evitare tagli\nplt.show()\n\n# Grafico per il Test Set\nplt.figure(figsize=(25, 9))\nsns.barplot(x='Classe', y='Numero Immagini', data=df_test_top, palette='plasma', hue='Classe')\nplt.title('Distribuzione Immagini per Classe nel Test Set', fontsize=28)\nplt.xlabel('Classe di Frutta', fontsize=22)\nplt.ylabel('Numero di Immagini', fontsize=22)\nplt.xticks(rotation=90, fontsize=18)\nplt.tight_layout()\nplt.show()","metadata":{"id":"BZaubGrJ6taH","outputId":"513c39fe-9ceb-44bf-9329-c09a636f5c12","trusted":true,"execution":{"iopub.status.busy":"2025-07-01T14:56:57.894196Z","iopub.execute_input":"2025-07-01T14:56:57.894507Z","iopub.status.idle":"2025-07-01T14:57:00.749220Z","shell.execute_reply.started":"2025-07-01T14:56:57.894484Z","shell.execute_reply":"2025-07-01T14:57:00.748410Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Visto il gran numero di classi selezioniamo solo le 5 più popolate nel training set in modo da avere un livello di dettaglio che ci permetta di osservare il rapporto tra gli elementi nei set di Train e di Test","metadata":{"id":"l0DAlmXfa33C"}},{"cell_type":"code","source":"# Kaggle notebooks genera numerosi warning a causa dell'uso di versioni precedenti di python\n# per cui gli andiamo a sopprimere\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Seleziono le 5 classi più popolose dal training set\ntop_5_train_classes = df_train.head(5)['Classe'].tolist()\n\n# Filtro entrambi i DataFrame per includere solo le 5 classi selezionate\ndf_train_filtered = df_train[df_train['Classe'].isin(top_5_train_classes)].copy()\ndf_test_filtered = df_test[df_test['Classe'].isin(top_5_train_classes)].copy()\n\n# Aggiungi una colonna 'Set' per identificare l'origine dei dati che poi unirò in un unico dataframe\ndf_train_filtered['Set'] = 'Train'\ndf_test_filtered['Set'] = 'Test'\n\n# Combino i due DataFrame filtrati in un unico DataFrame\ndf_combined_top5 = pd.concat([df_train_filtered, df_test_filtered])\n\n# Riordino le classi in base all'ordine del training set per la visualizzazione nel grafico\ndf_combined_top5['Classe'] = pd.Categorical(df_combined_top5['Classe'], categories=top_5_train_classes, ordered=True)\ndf_combined_top5 = df_combined_top5.sort_values('Classe')\n\nplt.figure(figsize=(12, 7))\n# Uso 'hue' per distinguere tra 'Train' e 'Test'\nsns.barplot(x='Classe', y='Numero Immagini', hue='Set', data=df_combined_top5, palette='viridis') # Palette automatica per hue\nplt.title('Distribuzione Immagini per le 5 Classi Top del Training Set (Train vs. Test)', fontsize=14)\nplt.xlabel('Classe di Frutta', fontsize=12)\nplt.ylabel('Numero di Immagini', fontsize=12)\nplt.xticks(rotation=45, ha='right', fontsize=10) # Ruota le etichette per leggibilità\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.legend(title='Dataset') # La leggenda viene generata automaticamente da hue\nplt.tight_layout()\nplt.show()","metadata":{"id":"vWhrE0O8U3ZV","outputId":"7264861a-4d63-4709-eba7-0033fd888971","trusted":true,"execution":{"iopub.status.busy":"2025-07-01T15:18:36.675527Z","iopub.execute_input":"2025-07-01T15:18:36.675866Z","iopub.status.idle":"2025-07-01T15:18:36.914700Z","shell.execute_reply.started":"2025-07-01T15:18:36.675828Z","shell.execute_reply":"2025-07-01T15:18:36.913971Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Riduzione dimensionale e classificazione visiva della nuvola di punti**\nNell'esplorazione ed analisi dei dati vogliamo ora vedere le singole entry del dataset come nuvola di punti per una stima visiva del loro raggruppamento in classi. Per visualizzare i diversi elementi, che nel nostro caso sono delle immagini, come una nuvola di punti (scatter plot) e far emergere il loro raggruppamento in classi, non possiamo semplicemente prendere le immagini così come sono in quanto sono dati ad alta dimensione (ad esempio, un'immagine 100x100 pixel RGB significa 30.000 dimensioni!), e non possiamo plottare un punto in uno spazio così grande.\n\nDobbiamo prima ridurre la dimensionalità delle immagini a 2 o 3 dimensioni, mantenendo il più possibile le relazioni spaziali tra i dati.\n\nPer fare questo usiamo la tecnica t-SNE ma vediamo prima alcune tecniche comuni:\n\n* Principal Component Analysis (PCA): Lineare, cerca di trovare le direzioni di massima varianza nei dati.\n* t-distributed Stochastic Neighbor Embedding (t-SNE): Non lineare, eccellente per visualizzare raggruppamenti di dati ad alta dimensione in uno spazio a bassa dimensione, preservando le vicinanze tra i punti. È spesso preferito per la visualizzazione.\n* Uniform Manifold Approximation and Projection (UMAP): Simile a t-SNE, spesso più veloce e scalabile, e talvolta produce raggruppamenti più densi.\n\nPer il nostro scopo, abbiamo scelto t-SNE in quanto ideale per la visualizzazione di raggruppamenti.","metadata":{"id":"AlzFCEtHbpo-"}},{"cell_type":"code","source":"# Su Kaggle notebooks non riesco a sopprimere i warning generati in questa sezione\n# ma fino all'addestramento funziona tutto anche su Google Colab sostituendo il path\n# del dataset\n\n# 4. Impostazione dei parametri per l'elaborazione della nuvola di punti\nIMAGE_SIZE = (64, 64) # Ridimensiona le immagini a una dimensione più gestibile\nMAX_IMAGES_PER_CLASS = 100 # Riduciamo il numero massimo di immagini da caricare per classe\nNUM_CLASSES_TO_PLOT = 10   # Limitiamo la visualizzazione a sole 10 classi\n\nprint(f\"Caricamento dati da: {train_dir}\")\nprint(f\"Ridimensionamento immagini a: {IMAGE_SIZE[0]}x{IMAGE_SIZE[1]}\")\nprint(f\"Max immagini per classe: {MAX_IMAGES_PER_CLASS}\\n\")\nprint(f\"Numero di classi da mostrare nel grafico: {NUM_CLASSES_TO_PLOT}\\n\")\n\n# 4.1. Caricamento e pre-processing di un sottoinsieme delle immagini\ndata = []\nlabels = []\nclass_names = sorted(os.listdir(train_dir))\nif not class_names:\n    print(f\"Errore: Nessuna classe trovata nella directory di training: {train_dir}\")\n    print(\"Controlla il percorso del dataset e assicurati che sia scompattato.\")\n    exit()\n\n# Selezioniamo solo le prime N classi per il nostro esempio\nclass_names_to_process = class_names[:NUM_CLASSES_TO_PLOT]\n\nprint(f\"Caricamento immagini dalle prime {NUM_CLASSES_TO_PLOT} classi: {class_names_to_process} in corso...\")\nstart_time = time.time()\n\nfor i, class_name in enumerate(class_names_to_process):\n    class_path = os.path.join(train_dir, class_name)\n    if os.path.isdir(class_path):\n        images_in_class = [img for img in os.listdir(class_path) if img.endswith(('.jpg', '.jpeg', '.png'))]\n        random.shuffle(images_in_class) # Mescola le entry per selezionare un sottoinsieme casuale\n\n        # Limita il numero di immagini per classe\n        for img_name in images_in_class[:MAX_IMAGES_PER_CLASS]:\n            img_path = os.path.join(class_path, img_name)\n            try:\n                img = Image.open(img_path).resize(IMAGE_SIZE).convert('RGB')\n                img_array = np.array(img) / 255.0 # Normalizziamo i pixel a [0, 1]\n                data.append(img_array.flatten()) # Appiattiamo l'immagine in un vettore\n                labels.append(class_name)\n            except Exception as e:\n                print(f\"Errore durante il caricamento di {img_path}: {e}\")\n    if (i + 1) % 10 == 0:\n        print(f\"  Processed {i + 1}/{len(class_names)} classes...\")\n\ndata = np.array(data)\nlabels = np.array(labels)\n\nprint(f\"\\nCaricamento completato in {time.time() - start_time:.2f} secondi.\")\nprint(f\"Caricate {len(data)} immagini totali.\")\nprint(f\"Dimensioni del dataset per t-SNE: {data.shape}\")\nprint(f\"Numero di classi caricate: {len(np.unique(labels))}\")\nprint(\"-\" * 50)\n\n# 4.2. Scaling dei dati (utile per t-SNE)\nprint(\"Scalatura dei dati...\")\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(data)\nprint(\"Scalatura completata.\")\nprint(\"-\" * 50)\n\n# 4.3. Applicazione di t-SNE per la riduzione di dimensionalità\nprint(\"Applicazione di t-SNE (potrebbe richiedere del tempo)...\")\n# n_components=2 per 2D, random_state per riproducibilità\n# perplexity e max_iter sono parametri importanti da impostare correttamente per ottenere una buona visualizzazione della nuvola di punti\n# tsne = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=1000, learning_rate=200)\n# riportato max_iter a n_iter su Kagge notebook\ntsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000, learning_rate=200)\ntsne_results = tsne.fit_transform(scaled_data)\nprint(\"t-SNE completato.\")\nprint(\"-\" * 50)\n\n# 4.4. Presentazione grafica della nuvola di punti\nplt.figure(figsize=(16, 12))\n\n# Mappiamo i nomi delle classi a valori numerici per la colorazione\nunique_labels = np.unique(labels)\nlabel_to_int = {label: i for i, label in enumerate(unique_labels)}\nnumeric_labels = np.array([label_to_int[label] for label in labels])\n\n# sns.scatterplot è ottimo per questo\n# Usiamo i punti ridotti da t-SNE, e il 'hue' per colorare per classe\nsns.scatterplot(\n    x=tsne_results[:, 0], y=tsne_results[:, 1],\n    hue=labels, # Usa i nomi delle classi originali per la leggenda\n    palette=sns.color_palette(\"tab10\", len(unique_labels)), # Palette per il numero di classi\n    legend=\"full\",\n    alpha=0.7,\n    s=20 # Dimensione dei punti\n)\n\nplt.title('Nuvola di Punti delle Immagini di Frutta (Riduzione Dimensionalità con t-SNE)', fontsize=18)\nplt.xlabel('Componente t-SNE 1', fontsize=14)\nplt.ylabel('Componente t-SNE 2', fontsize=14)\nplt.grid(True, linestyle='--', alpha=0.6)\n\n# Posizioniamo la legenda fuori dal grafico se ci sono molte classi\nif len(unique_labels) > 20:\n    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\nplt.tight_layout() # Aggiusta layout per evitare sovrapposizioni\nplt.show()","metadata":{"id":"o-WNcayddEko","outputId":"840a2ffe-cb52-4418-a5c6-193e32140955","trusted":true,"execution":{"iopub.status.busy":"2025-07-01T16:04:22.670619Z","iopub.execute_input":"2025-07-01T16:04:22.671275Z","iopub.status.idle":"2025-07-01T16:04:39.345989Z","shell.execute_reply.started":"2025-07-01T16:04:22.671245Z","shell.execute_reply":"2025-07-01T16:04:39.345225Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Nuvola 3D**\nVista la visualizzazione della distribuzione in 2D, che mostra evidenti pattern ma che vediamo non bastare a spiegare la categorizzazione finale, aggiungiamo una dimensione in quanto evidentemente non stiamo considerando qualche feature importante nell'analisi della correlazione.\n(Ci renderemo conto che anche in questo caso non abbiamo una visualizzazione ottimale ma nuovamente vediamo dei pattern riconoscibili)","metadata":{"id":"pfUVeqftGvXB"}},{"cell_type":"code","source":"# 4.3.b Applicazione di t-SNE per la riduzione di dimensionalità\nprint(\"Applicazione di t-SNE (potrebbe richiedere del tempo)...\")\n# n_components=3 per 3D, random_state per riproducibilità\n# perplexity e max_iter sono parametri importanti da impostare correttamente per ottenere una buona visualizzazione della nuvola di punti\n# tsne = TSNE(n_components=3, random_state=42, perplexity=30, max_iter=1000, learning_rate=200)\n# riportato max_iter a n_iter su Kagge notebook\ntsne = TSNE(n_components=3, random_state=42, perplexity=30, n_iter=1000, learning_rate=200)\ntsne_results = tsne.fit_transform(scaled_data)\nprint(\"t-SNE 3D completato.\")\nprint(\"-\" * 50)\n\n# 4.4.b Presentazione grafica della nuvola di punti\nplt.figure(figsize=(16, 12))\nax = plt.figure().add_subplot(projection='3d') # CREA UN SOTTO-GRAFICO 3D\n\n# Ottieniamo le classi uniche e generiamo una palette di colori\nunique_labels = np.unique(labels)\ncolors = sns.color_palette(\"tab10\", len(unique_labels)) # Ottieniamo i colori da Seaborn\nlabel_color_map = dict(zip(unique_labels, colors)) # Mappiamo le etichetta ai colori\n\n# Visualizziamo ogni punto assegnando il colore in base alla sua classe\nfor i, label in enumerate(unique_labels):\n    # Selezioniamo i punti che appartengono alla classe corrente\n    indices = np.where(labels == label)\n    ax.scatter(\n        tsne_results[indices, 0],\n        tsne_results[indices, 1],\n        tsne_results[indices, 2],\n        color=label_color_map[label], # Usiamo il colore mappato\n        label=label, # Etichettiamo per la legenda\n        alpha=0.7,\n        s=20\n    )\n\nax.set_title(f'Nuvola di Punti delle Immagini di Frutta (Top {NUM_CLASSES_TO_PLOT} Classi con t-SNE 3D)', fontsize=18)\nplt.grid(True, linestyle='--', alpha=0.6)\n\n# Posizioniamo la legenda fuori dal grafico\nax.legend(title='Classe di Frutta', bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0.)\n\nplt.tight_layout()\nplt.show()","metadata":{"id":"FmEBbuzDGxDp","outputId":"38b581b0-ec8d-4421-cb01-1ef81d865814","trusted":true,"execution":{"iopub.status.busy":"2025-07-01T15:34:18.890997Z","iopub.execute_input":"2025-07-01T15:34:18.891260Z","iopub.status.idle":"2025-07-01T15:34:29.150192Z","shell.execute_reply.started":"2025-07-01T15:34:18.891244Z","shell.execute_reply":"2025-07-01T15:34:29.149422Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Data Augmentation**\nSfruttando Keras con ImageDataGenerator procediamo a creare in tempo reale delle varianti delle immagini al fine di estendere il dataset fornito per l'addestramento della nostra IA.","metadata":{"id":"FHJ2dlaVltEQ"}},{"cell_type":"markdown","source":"Nel codice seguente useremo ImageDataGenerator per le seguenti trasformazioni:\n\n* rescale=1./255: Normalizza i valori dei pixel da [0, 255] a [0, 1];\n\n* rotation_range:Applica leggere rotazioni;\n\n* width_shift_range, height_shift_range: Effettua degli spostamenti;\n\n* shear_range: Applica delle deformazioni;\n\n* zoom_range: Zoom in/out sulle immagini;\n\n* horizontal_flip=True: Effettua il ribaltamento orizzontale delle immagini;\n\n* fill_mode='nearest': Indica la modalità con cui riempire i pixel creati dalle trasformazioni.\n\nCon le immagini generate usiamo flow_from_directory per caricare le immagini e assegnare le etichette. Specifichiamo il target_size (ad esempio (100, 100)), batch_size, class_mode='categorical'.","metadata":{"id":"kKSIUIwWoh4C"}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# 5. Impostiamo i parametri per ImageDataGenerator e caricamento dati\nTARGET_SIZE = (100, 100) # Dimensione a cui verranno ridimensionate tutte le immagini\nBATCH_SIZE = 32          # Numero di immagini per batch\n\n# 5.2. Procediamo con la creazione di ImageDataGenerator per il Training Set con Data Augmentation\n# con le trasformazioni previste\nprint(\"Creazione del generatore di immagini per il training set...\")\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,                 # Normalizza i pixel da [0, 255] a [0, 1]\n    rotation_range=40,              # Ruota casualmente le immagini fino a 40 gradi\n    width_shift_range=0.2,          # Sposta casualmente le immagini orizzontalmente (frazione della larghezza totale)\n    height_shift_range=0.2,         # Sposta casualmente le immagini verticalmente (frazione dell'altezza totale)\n    shear_range=0.2,                # Applica delle trasformazioni di \"taglio\" (shear transformation)\n    zoom_range=0.2,                 # Applica zoom casuale all'interno delle immagini\n    horizontal_flip=True,           # Ribalta casualmente le immagini orizzontalmente\n    fill_mode='nearest'             # Strategia per riempire i pixel nuovi creati dalle trasformazioni (es. rotazioni)\n                                    # 'nearest' riempie con il valore del pixel più vicino\n)\n\n# 5.3. Carichiamo le immagini generate dal Training Directory\nprint(f\"Caricamento immagini dal percorso: {train_dir}\")\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,                      # Percorso della directory di training\n    target_size=TARGET_SIZE,        # Ridimensiona tutte le immagini a questa dimensione\n    batch_size=BATCH_SIZE,          # Numero di immagini da restituire per ogni batch\n    class_mode='categorical',       # Le etichette saranno in formato one-hot encoded (necessario per classificazione multi-classe)\n    shuffle=True                    # Mescola l'ordine delle immagini per ogni epoca\n)\n\nprint(\"\\nGeneratore per il training set configurato con successo!\")\nprint(f\"Numero di classi trovate: {train_generator.num_classes}\")\nprint(f\"Nomi delle classi: {train_generator.class_indices}\") # Mostra la mappatura tra nomi classi e indici numerici\n\n# 5.4. Esempio: Visualizziamo alcuni batch di immagini per il Training generate\n# Con questo blocco verifichiamo che le trasformazioni funzionino come previsto\n# import matplotlib.pyplot as plt\n\nprint(\"\\nVisualizzazione di alcune immagini trasformate (può richiedere qualche secondo)...\")\nplt.figure(figsize=(10, 10))\nfor images, labels in train_generator:\n    for i in range(min(9, BATCH_SIZE)): # Visualizza fino a 9 immagini dal primo batch\n        ax = plt.subplot(3, 3, i + 1)\n        # ImageDataGenerator restituisce i pixel già in [0,1], quindi moltiplica per 255 se vuoi visualizzare\n        # con imshow (che si aspetta valori in [0,1] o [0,255])\n        plt.imshow(images[i])\n        # Puoi provare a decodificare la label one-hot per visualizzare il nome della classe\n        current_label_index = tf.argmax(labels[i]).numpy()\n        current_label_name = list(train_generator.class_indices.keys())[list(train_generator.class_indices.values()).index(current_label_index)]\n        plt.title(current_label_name)\n        plt.axis(\"off\")\n    break # Interrompi dopo il primo batch\nplt.tight_layout()\nplt.show()\n\n# 5.5 Creazione del generatore per il Test/Validation Set (SENZA data augmentation)\n# È fondamentale NON applicare data augmentation al set di test/validazione.\n# Vogliamo valutare il modello su immagini \"pulite\" e non alterate.\nprint(\"\\nCreazione del generatore di immagini per il test/validation set (solo rescale)...\")\ntest_datagen = ImageDataGenerator(rescale=1./255) # Solo normalizzazione\n\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,                       # Percorso della directory di test\n    target_size=TARGET_SIZE,\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    shuffle=False                   # Non mescolare il test set per avere risultati riproducibili\n)\n\nprint(\"\\nGeneratore per il test set configurato con successo!\")","metadata":{"id":"mvs-EW1Dm-_m","outputId":"13d1030b-d7a0-4892-93aa-77ab3dedfb8f"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Definizione del modello**\nTerminate le operazioni sui dati ci dedichiamo al modello. Affrontando un problema di classificazione useremo una CNN che è particolarmente adatta alle nostre necessità essendo un'architettura di deep learning particolarmente adatta all'analisi di dati visuali, come le immagini.","metadata":{"id":"2SidXsAztH9p"}},{"cell_type":"markdown","source":"Nel codice seguente definiremo l'architettura della nostra CNN utilizzando l'API Sequenziale di Keras","metadata":{"id":"MVHU_DzkuVO3"}},{"cell_type":"code","source":"# 6. Definizione del modello\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n\n# 6.1 Impostazione dei parametri (Non vengono reimpostati quelli già impostati nei codici precedenti)\nIMAGE_CHANNELS = 3       # 3 per i canali RGB delle immagini a colori\nNUM_CLASSES = train_generator.num_classes\n\nprint(f\"Dimensioni attese delle immagini in input: {TARGET_SIZE[0]}x{TARGET_SIZE[1]} con {IMAGE_CHANNELS} canali.\")\nprint(f\"Numero di classi per l'output: {NUM_CLASSES}\")\nprint(\"-\" * 50)\n\n# 6.2. Architettura di Base della CNN\nprint(\"Definizione dell'architettura della CNN...\")\n\nmodel = Sequential([\n    # Primo blocco Convoluzionale e Pooling\n    Conv2D(32, (3, 3), activation='relu', input_shape=(TARGET_SIZE[0], TARGET_SIZE[1], IMAGE_CHANNELS)),\n    # Conv2D definisce lo strato della nostra rete ricevendo informazioni sul numero di feature da apprendere,\n    # sulla dimensione della finestra convoluzionale che scansiona l'immagine, sulla funzione di attivazione\n    # (usiamo la ReLU perché aiuta il modello ad apprendere relazioni non lineari ed è computazionalmente efficiente) e,\n    # solo nel primo srato, le caratteristiche delle immagini da analizzare.\n    MaxPooling2D((2, 2)),\n    # Riduce la dimensione spaziale della feature map, aiuta a ridurre il numero di parametri, a rendere il modello\n    # più robusto a piccole variazioni nella posizione degli oggetti e ad evitare l'overfitting.\n\n    # Secondo blocco Convoluzionale e Pooling\n    Conv2D(64, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n\n    # Terzo blocco Convoluzionale e Pooling\n    Conv2D(128, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n\n    # Quarto blocco Convoluzionale e Pooling (opzionale, ma utile per feature più complesse)\n    Conv2D(256, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n\n    # Strato di Dropout per prevenire l'overfitting\n    Dropout(0.25),\n    # Disattiva casualmente una percentuale (es. 25%) di neuroni in ogni passo di addestramento.\n    # Questo simula l'addestramento di un insieme di reti sparse, forzando i neuroni ad apprendere percorsi\n    # alternativi e a ridurre le co-dipendenze. Il risultato è un modello più robusto, che userò tutti i\n    # neuroni disponibili durante la predizione, ma che è meno soggetto all'overfitting.\n\n    # Appiattisce l'output degli strati convoluzionali per passarlo agli strati Dense\n    Flatten(),\n    # Converte l'outpud in 3 dimensioni (altezza, larghezza e filtri) in uno monodimensionale\n\n    # Strati Fully Connected (Dense) di una Rete Neutrale a Percettroni Multistrato\n    Dense(512, activation='relu'), # Uno strato nascosto\n    Dropout(0.5),                  # Un altro strato di Dropout\n\n    # Strato di output finale\n    Dense(NUM_CLASSES, activation='softmax') # Il numero di neuroni deve essere uguale al numero di classi\n                                             # 'softmax' per classificazione multi-classe:\n                                             # converte gli output grezzi del neurone in probabilità che l'immagine\n                                             # appartenga a ciascuna classe.\n])\n\nprint(\"Architettura della CNN definita.\")\nprint(\"-\" * 50)","metadata":{"id":"pvAd4o6BuKpT","outputId":"a0f6171f-6c5e-4644-d0b8-db0d05596ea1"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Una volta definita l'architettura del modello dobbiamo compilarlo assegnando una funzione di Loss ed una di ottimizzazione per poter addestrare il modello.\nPer la tipologia di classificazione multi-classe in cui un oggetto appartiene o no ad una categoria, abbiamo selezionato l'ottimizzatore Adam e la funzione ci Loss CCE (Categorical Cross Entropy).\n\n**Ottimizzatore**\n\nL'ottimizzatore Adam si presta bene alla nostra esercitazione per l'efficienza computazionale e per la poca necessità di personalizzazione dei parametri, funzionando generalmente bene con le impostazioni di default.\n\n**Funzione di Loss**\n\nLa funzione di Loss CCE da la misura di quanto una previsione sia errata, di quanto si discosti dalle Ground Truth (Le etichette fornite).\nCerchiamo di immaginare il funzionamento del nostro modello:\n\n  Data un'immagine di mela, il modello produce delle probabilità per le tre classi:\n  * Previsione del modello: [0.8, 0.1, 0.1] (80% mela, 10% banana, 10% arancia)\n  * Etichetta reale (one-hot): [1.0, 0.0, 0.0] (è una mela)\n\nLa CCE calcola una penalità basata su quanto le probabilità previste per le classi sbagliate sono alte, e quanto la probabilità prevista per la classe corretta è bassa.\n\nLa formula è: L=−∑\\[i=1 -> C\\](yi*log(^yi))\n\nC indica il numero delle classi, yi la ground truth (1 o 0), ^yi la nostra predizione.\n\n*Il logaritmo fa si che quanto più la predizione si avvicina ad 1, tanto più il suo logaritmo si avvicina a 0 risultando meno penalizzante.*\n","metadata":{"id":"b7UfAgTpULAR"}},{"cell_type":"code","source":"# 6.3. Compilazione del Modello\nprint(\"Compilazione del modello...\")\nmodel.compile(\n    optimizer='adam',                   # Ottimizzatore Adam (Adaptive Moment Estimation), efficiente computazionalmente\n                                        # e generalmente ben funzionante con parametri di default\n    loss='categorical_crossentropy',    # Funzione di perdita per classificazione multi-classe one-hot encoded (Appartenenti o no alla classe)\n    metrics=['accuracy']                # Metriche da monitorare durante l'addestramento (precisione)\n)\n\nprint(\"Modello compilato con successo!\")\nprint(\"-\" * 50)\n\n# 6.4. Riepilogo del Modello\nprint(\"Riepilogo dell'architettura del modello (model.summary()):\\n\")\nmodel.summary()\n\nprint(\"\\nDefinizione e compilazione del modello completate!\")","metadata":{"id":"sz5r6NKIUI9I","outputId":"bc450e73-e4f0-45c8-ea39-ca67babd713b"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Addestramento del Modello**\nIn questa fase, alimenteremo il modello con i dati di training e useremo il set di validazione per monitorare le sue prestazioni e prevenire l'overfitting.\n\nUseremo anche i Callbacks per salvare il modello migliore e fermare l'addestramento se non ci sono più miglioramenti, evitando di andare in overfitting.","metadata":{"id":"NcKxOVOjZAR-"}},{"cell_type":"code","source":"# 7. Addestramento del modello\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n\n# 7.1 Parametri per l'addestramento\nEPOCHS = 50 # Numero di epoche. Iniziamo con un numero ragionevole ed usiamo EarlyStopping.\n\n# Percorso per salvare il modello migliore\ncheckpoint_filepath = '/content/drive/MyDrive/FruitDLProj/best_fruit_classifier_model.keras' # Salva in .keras format\n\nprint(f\"Inizio addestramento del modello per {EPOCHS} epoche.\")\nprint(f\"Il modello migliore verrà salvato in: {checkpoint_filepath}\")\nprint(\"-\" * 50)\n\n# 7.2. Callbacks\n\n# 7.2.1. ModelCheckpoint: Salva il modello migliore\n# Monitora la 'val_accuracy' (accuratezza sul set di validazione)\n# mode='max' significa che cerchiamo di massimizzare questo valore\n# save_best_only=True assicura che venga salvata solo la versione migliore del modello\n# verbose=1 per mostrare un messaggio quando il modello viene salvato\ncheckpoint_callback = ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    monitor='val_accuracy',\n    mode='max',\n    save_best_only=True,\n    verbose=1\n)\n\n# 7.2.2. EarlyStopping: Ferma l'addestramento in anticipo se non ci sono miglioramenti\n# monitor='val_accuracy' come per ModelCheckpoint\n# patience=10 significa che aspetterà 10 epoche senza miglioramenti prima di fermarsi\n# restore_best_weights=True ripristina i pesi del modello alla migliore epoca trovata\n# verbose=1 per mostrare un messaggio quando l'addestramento si ferma\nearly_stopping_callback = EarlyStopping(\n    monitor='val_accuracy',\n    patience=10,\n    restore_best_weights=True,\n    verbose=1\n)\n\n# Lista di callbacks da passare a model.fit\ncallbacks_list = [checkpoint_callback, early_stopping_callback]\n\n# 7.3. Addestramento Effettivo del Modello (model.fit())\nhistory = model.fit(\n    train_generator,                       # Il generatore di dati per il training\n    epochs=EPOCHS,                         # Il numero massimo di epoche\n    validation_data=test_generator,        # Il generatore di dati per la validazione (test set)\n    callbacks=callbacks_list               # La lista dei callbacks definiti\n    # steps_per_epoch e validation_steps possono essere omessi con flow_from_directory,\n    # Keras li calcola automaticamente in base al numero di campioni e al batch_size.\n)\n\nprint(\"\\nAddestramento del modello completato!\")\nprint(\"-\" * 50)\n\n# 7.4. Visualizzazione dell'andamento dell'addestramento\nprint(\"Visualizzazione dell'andamento di accuratezza e loss...\")\n\n# 7.4.1. Accedi alla cronologia dell'addestramento\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(len(acc))\n\nplt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 1) # Primo subplot per l'accuratezza\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epoca')\nplt.ylabel('Accuratezza')\n\nplt.subplot(1, 2, 2) # Secondo subplot per la loss\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoca')\nplt.ylabel('Loss')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nAddestramento completato e cronologia visualizzata.\")","metadata":{"id":"azuDDThVaByU","outputId":"91a7086f-1357-45b5-8ce3-c9b0c1710235"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Aggregazioni e semplificazioni per l'esercitazione**\nLe risorse, anche acquistando le GPU A100, su Google Colab, con i dataset attuali richiederebbero oltre una settimana per l'addestramento, per cui, per riuscire a proporre il progetto ho valutato la necessità di ridurre la granularità di riconoscimento del modello, distinguere una mela da una banana ma non tra mela golden, mela cavendish, ecc.\n\nPer ottenere questo risultato, prima di procedere con l'addestramento faremo aggregazioni ed un alleggerimento del dataset, limitando a 10 le classi desiderate, per poter raggiungere risultati osservabili in tempi appropriati alla portata del progetto, in vista del prossimo esame di luglio.\n\nDovremo modificare il codice che prepara i generatori di immagini (ImageDataGenerator.flow_from_directory) e che definisce il numero di classi nel modello (NUM_CLASSES), rispetto a quanto visto in precedenza.\n\nDopo una prima fase di test si sono rese necessarie ulteriori semplificazioni limitando fortemente il numero delle classi (scelte da me) e delle foto massime per classe.","metadata":{"id":"VsosS44MTTxT"}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n\n# 8. Funzione per aggregare e filtrare i nomi delle classi\n# Questa funzione prende il nome di una sottocartella (es. 'Apple Braeburn')\n# e restituisce il nome aggregato (es. 'Apple') filtrando solo le classi desiderate\n\ndef aggregate_and_filter_fruit_name(folder_name, desired_classes_set):\n    # Prende la prima parola come nome generico del frutto\n    aggregated_name = folder_name.split(' ')[0]\n    if aggregated_name == 'Walnut': # Se il dataset ha \"Walnut\", lo mappiamo a \"nut\"\n        aggregated_name = 'Nut'\n    elif aggregated_name == 'Hazelnut': # Se il dataset ha \"Hazelnut\", lo mappiamo a \"nut\"\n        aggregated_name = 'Nut'\n\n    # Restituisce il nome aggregato solo se è nella lista delle classi desiderate\n    if aggregated_name.lower() in desired_classes_set:\n        return aggregated_name.lower() # Restituisce in minuscolo per coerenza\n    else:\n        return None # Indica che questa classe non deve essere inclusa\n\n# 8.1. Definizione delle 10 classi desiderate\n# Le classi saranno in minuscolo per facilitare il confronto\nDESIRED_CLASSES = [\n    \"apple\", \"banana\", \"cherry\", \"nut\", \"orange\", \"strawberry\"\n]\nDESIRED_CLASSES_SET = set(DESIRED_CLASSES) # Per lookup più efficienti\nNUM_AGGREGATED_CLASSES = len(DESIRED_CLASSES)\n\nprint(f\"Classi aggregate desiderate ({NUM_AGGREGATED_CLASSES}): {DESIRED_CLASSES}\")\nprint(\"-\" * 50)\n\n# Limite immagini per classe\n# Questo parametro fondamentale per ridurre il tempo di addestramento.\n# Ogni classe avrà al massimo questo numero di immagini.\nMAX_IMAGES_PER_CLASS_LIMIT = 150 # Puoi sperimentare con 200, 250, 300, 400\n\n# 8.2. Preparazione dei dati per ImageDataGenerator con mappatura e filtraggio\ndef create_dataframe_for_generator_filtered(base_dir, class_aggregator_func, desired_classes_set, max_images_limit):\n    filepaths = []\n    labels = []\n    for original_class_folder in os.listdir(base_dir):\n        original_class_path = os.path.join(base_dir, original_class_folder)\n        if os.path.isdir(original_class_path):\n            aggregated_label = class_aggregator_func(original_class_folder, desired_classes_set)\n            if aggregated_label is not None:\n                # Ottieni tutti i nomi dei file immagine in questa sottocartella\n                img_names = [img for img in os.listdir(original_class_path) if img.lower().endswith(('.jpg', '.jpeg', '.png'))]\n\n                # Seleziona un sottoinsieme casuale di immagini se il conteggio supera il limite\n                if len(img_names) > max_images_limit:\n                    img_names = random.sample(img_names, max_images_limit) # Seleziona casualmente il numero massimo\n\n                for img_name in img_names:\n                    filepaths.append(os.path.join(original_class_path, img_name))\n                    labels.append(aggregated_label)\n    return pd.DataFrame({'filepaths': filepaths, 'labels': labels})\n\n# Creazione dei DataFrames per train e test, ora filtrati e limitati\ndf_train = create_dataframe_for_generator_filtered(train_dir, aggregate_and_filter_fruit_name, DESIRED_CLASSES_SET, MAX_IMAGES_PER_CLASS_LIMIT)\ndf_test = create_dataframe_for_generator_filtered(test_dir, aggregate_and_filter_fruit_name, DESIRED_CLASSES_SET, MAX_IMAGES_PER_CLASS_LIMIT)\n\n\nprint(f\"DataFrame di training creato con {len(df_train)} immagini (limitato a {MAX_IMAGES_PER_CLASS_LIMIT} per classe).\")\nprint(f\"DataFrame di test creato con {len(df_test)} immagini (limitato a {MAX_IMAGES_PER_CLASS_LIMIT} per classe).\")\nprint(\"-\" * 50)\n\n# 8.3. ImageDataGenerator per il Training Set (con data augmentation) ---\nTARGET_SIZE = (72, 72)\nBATCH_SIZE = 16\nEPOCHS = 6 # Riduciamo le epoche visto il problema semplificato\n\nprint(\"Creazione del generatore di immagini per il training set...\")\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\ntrain_generator = train_datagen.flow_from_dataframe(\n    dataframe=df_train,\n    x_col='filepaths',\n    y_col='labels',\n    target_size=TARGET_SIZE,\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    classes=DESIRED_CLASSES, # Usiamo la lista esplicita delle classi desiderate\n    shuffle=True\n)\n\nprint(\"\\nGeneratore per il training set configurato con successo!\")\nprint(f\"Numero di classi aggregate trovate: {len(train_generator.classes)}\")\nprint(f\"Nomi delle classi aggregate: {train_generator.class_indices}\")\nprint(\"-\" * 50)\n\n# 8.4. ImageDataGenerator per il Test/Validation Set (solo rescale)\nprint(\"Creazione del generatore di immagini per il test/validation set (solo rescale)...\")\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntest_generator = test_datagen.flow_from_dataframe(\n    dataframe=df_test,\n    x_col='filepaths',\n    y_col='labels',\n    target_size=TARGET_SIZE,\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    classes=DESIRED_CLASSES, # Usiamo la lista esplicita delle classi desiderate\n    shuffle=False\n)\n\nprint(\"\\nGeneratore per il test set configurato con successo!\")\nprint(\"-\" * 50)\n\n# 8.5. Ridefinizione del Modello CNN\n# Adattiamo NUM_CLASSES al nuovo numero di classi aggregate\nIMAGE_CHANNELS = 3\n\nprint(\"Definizione dell'architettura della CNN (adattata alle classi aggregate)...\")\nmodel = Sequential([\n    Conv2D(32, (3, 3), activation='relu', input_shape=(TARGET_SIZE[0], TARGET_SIZE[1], IMAGE_CHANNELS)),\n    MaxPooling2D((2, 2)),\n    Conv2D(64, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n    Conv2D(128, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n    Conv2D(256, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n    Dropout(0.25),\n    Flatten(),\n    Dense(512, activation='relu'),\n    Dropout(0.5),\n    Dense(NUM_AGGREGATED_CLASSES, activation='softmax') # USIAMO IL NUOVO NUMERO DI CLASSI AGGREGATE\n])\n\nprint(\"Architettura della CNN definita.\")\nprint(\"-\" * 50)\n\n# 8.6. Ricompilazione del Modello\nprint(\"Compilazione del modello...\")\nmodel.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\nprint(\"Modello compilato con successo!\")\nprint(\"-\" * 50)\n\n# 8.7. Riepilogo del nuovo Modello\nprint(\"Riepilogo dell'architettura del modello (model.summary()):\\n\")\nmodel.summary()\nprint(\"\\nDefinizione e compilazione del modello completate!\")\nprint(\"-\" * 50)\n\n\n# 8.8. Callbacks per l'Addestramento\n# Patch Google Colab\n# checkpoint_filepath = '/content/drive/MyDrive/FruitDLProj/best_fruit_classifier_model.keras'\ncheckpoint_filepath = '/kaggle/working/best_fruit_classifier_model.keras'\n\ncheckpoint_callback = ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    monitor='val_accuracy',\n    mode='max',\n    save_best_only=True,\n    verbose=1\n)\n\nearly_stopping_callback = EarlyStopping(\n    monitor='val_accuracy',\n    patience=3, # Abbassiamo per raggiungere prima possibile uno stato di operabilità per i test\n    restore_best_weights=True,\n    verbose=1\n)\n\ncallbacks_list = [checkpoint_callback, early_stopping_callback]\n\n# 8.9. Addestramento Effettivo del Modello\nprint(\"Inizio addestramento del modello aggregato...\")\nhistory = model.fit(\n    train_generator,\n    epochs=EPOCHS,\n    validation_data=test_generator,\n    callbacks=callbacks_list\n)\n\nprint(\"\\nAddestramento del modello aggregato completato!\")\nprint(\"-\" * 50)\n\n# 8.10. Visualizzazione dell'andamento dell'addestramento\nprint(\"Visualizzazione dell'andamento di accuratezza e loss per il modello aggregato...\")\n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(len(acc))\n\nplt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy (Aggregated Classes)')\nplt.xlabel('Epoca')\nplt.ylabel('Accuratezza')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss (Aggregated Classes)')\nplt.xlabel('Epoca')\nplt.ylabel('Loss')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nAddestramento completato e cronologia visualizzata per le classi aggregate.\")","metadata":{"id":"An5ZF8w_UnCI","outputId":"32d219d7-9cc8-4f98-8051-d28a88e442b1","trusted":true,"execution":{"iopub.status.busy":"2025-07-01T16:04:49.049905Z","iopub.execute_input":"2025-07-01T16:04:49.050412Z","iopub.status.idle":"2025-07-01T16:11:05.047075Z","shell.execute_reply.started":"2025-07-01T16:04:49.050387Z","shell.execute_reply":"2025-07-01T16:11:05.046450Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Dal grafico si evince che alla sesta epoca il modello iniziava ad andare in overfitting, diminuendo il Loss sul set di training ma aumentando sul set di test per cui il modello che generalizza meglio e che è stato scelto è quello della quinta epoca.","metadata":{}},{"cell_type":"markdown","source":"# **Valutazione del Modello**\n","metadata":{}},{"cell_type":"code","source":"# 9. Caricamento del Modello Migliore\ncheckpoint_filepath = '/kaggle/working/best_fruit_classifier_model.keras'\n\n# Verifica se il file del modello esiste prima di caricarlo\nif os.path.exists(checkpoint_filepath):\n    print(f\"Caricamento del modello migliore da: {checkpoint_filepath}\")\n    model = tf.keras.models.load_model(checkpoint_filepath)\n    print(\"Modello caricato con successo!\")\nelse:\n    print(f\"Attenzione: Il modello non è stato trovato in {checkpoint_filepath}.\")\n    print(\"Assicurati di aver addestrato il modello e che il checkpoint sia corretto.\")\n    print(\"Se hai appena finito l'addestramento, il 'model' dovrebbe essere già quello migliore.\")\n    # Se il modello non viene trovato e non è già in memoria, le prossime operazioni falliranno.\n    # In un contesto reale, qui si dovrebbe gestire un errore o riaddestrare.\n\nprint(\"-\" * 50)\n\n# 9.1. Valutazione sul Test Set\n\nif 'test_generator' in locals(): # Verifica se test_generator esiste già\n    print(\"Valutazione del modello sul set di test...\")\n    # model.evaluate() restituisce la loss e le metriche (es. accuratezza)\n    # Calcola automaticamente steps (total_samples / batch_size)\n    loss, accuracy = model.evaluate(test_generator)\n\n    print(f\"\\n--- Risultati della Valutazione sul Test Set ---\")\n    print(f\"Test Loss: {loss:.4f}\")\n    print(f\"Test Accuracy: {accuracy:.4f}\")\n    print(\"---------------------------------------------\")\n\nelse:\n    print(\"Errore: 'test_generator' non è stato trovato.\")\n    print(\"Assicurati di aver eseguito le celle di preparazione dei dati che creano 'test_generator'.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T16:45:59.606584Z","iopub.execute_input":"2025-07-01T16:45:59.607275Z","iopub.status.idle":"2025-07-01T16:46:18.095283Z","shell.execute_reply.started":"2025-07-01T16:45:59.607250Z","shell.execute_reply":"2025-07-01T16:46:18.094675Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Predizioni su nuove immagini**\nIl modello prenderà le immagini inserite in uno specifico path, le preprocesserà ed elaborerà per assegnar loro una classe tra quelle per cui è addestrato.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom PIL import Image\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\n\n# 10.1 Caricamento del Modello Addestrato\n# Path del modello creato su Kaggle Notebook\nmodel_path = '/kaggle/working/best_fruit_classifier_model.keras'\n\ntry:\n    model = tf.keras.models.load_model(model_path)\n    print(f\"Modello caricato con successo da: {model_path}\")\nexcept Exception as e:\n    print(f\"Errore durante il caricamento del modello: {e}\")\n    print(\"Assicurati che il percorso del modello sia corretto e che il file esista.\")\n    # Esci o gestisci l'errore se il modello non può essere caricato\n    exit()\n\nprint(\"-\" * 50)\n\n# 10.2. Parametri di Pre-processing (devono corrispondere a quelli del training) ---\nTARGET_SIZE = (100, 100) # Dimensione a cui le immagini sono state ridimensionate durante l'addestramento\n\n\n# 10.3 Definizione delle Classi (nell'ordine in cui il modello le ha apprese)\nCLASS_NAMES = [\n    \"apple\", \"banana\", \"cherry\", \"nut\", \"orange\", \"strawberry\"\n]\n\nprint(f\"Il modello classificherà tra le seguenti {len(CLASS_NAMES)} classi: {CLASS_NAMES}\")\nprint(\"-\" * 50)\n\n# 10.4. Cartella delle Immagini da Classificare\n# Crea una cartella in Google Drive e caricali le immagini che vuoi testare.\n# Esempio: \"/content/drive/MyDrive/test_images_for_prediction\"\nimage_folder_path = '/content/drive/MyDrive/test_images_for_prediction' # <-- MODIFICA QUESTO PERCORSO\n\nif not os.path.exists(image_folder_path):\n    print(f\"Errore: La cartella '{image_folder_path}' non esiste.\")\n    print(\"Crea la cartella e carica le immagini al suo interno.\")\n    exit()\n\n# --- 5. Funzione per Pre-processare una singola immagine ---\ndef preprocess_image(image_path, target_size):\n    img = Image.open(image_path).convert('RGB') # Assicurati che sia RGB\n    img = img.resize(target_size) # Ridimensiona\n    img_array = np.array(img) # Converti in array NumPy\n    img_array = img_array / 255.0 # Normalizza i pixel a [0, 1]\n    img_array = np.expand_dims(img_array, axis=0) # Aggiungi la dimensione del batch (1, H, W, C)\n    return img_array\n\n# --- 6. Esecuzione delle Previsioni ---\nprint(f\"Inizio delle previsioni per le immagini in: {image_folder_path}\")\nplt.figure(figsize=(15, 15))\nplot_count = 0\n\n# Itera su tutti i file nella cartella specificata\nfor filename in os.listdir(image_folder_path):\n    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n        image_path = os.path.join(image_folder_path, filename)\n        \n        # Pre-processa l'immagine\n        processed_image = preprocess_image(image_path, TARGET_SIZE)\n        \n        # Esegui la previsione\n        predictions = model.predict(processed_image)\n        \n        # Ottieni la classe prevista (indice della probabilità più alta)\n        predicted_class_index = np.argmax(predictions[0])\n        predicted_class_name = CLASS_NAMES[predicted_class_index]\n        \n        # Ottieni la probabilità associata alla classe prevista\n        confidence = predictions[0][predicted_class_index]\n        \n        print(f\"Immagine: {filename}\")\n        print(f\"  Previsto: {predicted_class_name} (Confidenza: {confidence:.2f})\")\n        \n        # --- Visualizzazione ---\n        plot_count += 1\n        ax = plt.subplot(4, 4, plot_count) # Adatta la griglia a quante immagini vuoi mostrare\n                                          # Es. 4x4 per max 16 immagini\n        original_img = Image.open(image_path) # Carica l'immagine originale per la visualizzazione\n        plt.imshow(original_img)\n        plt.title(f\"{predicted_class_name}\\n({confidence*100:.1f}%)\", fontsize=10, color='green' if confidence > 0.7 else 'red')\n        plt.axis('off')\n\n        if plot_count >= 16: # Limita il numero di immagini visualizzate per evitare grafici enormi\n            print(\"Visualizzazione limitata a 16 immagini. Elaborando le rimanenti senza mostrare il grafico.\")\n            break # Rimuovi questo break se vuoi visualizzare tutte le immagini\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nPrevisioni completate per le immagini nella cartella.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **CPU vs GPU**\n\nMi sono reso conto con lo stato del sistema che, a dispetto delle impostazioni, sta addestrando sulla CPU. Verifichiamo col prossimo blocco l'uso della GPU con alcune configurazioni.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\n# Verifica se TensorFlow ha rilevato una GPU\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        # Configura la crescita della memoria (sebbene non sia fondamentale è una buona pratica)\n        tf.config.experimental.set_memory_growth(gpus[0], True)\n        print(f\"GPU rilevata: {gpus[0]}\")\n        print(\"Il tuo modello verrà addestrato sulla GPU.\")\n    except RuntimeError as e:\n        # Errore di inizializzazione della GPU\n        print(e)\n        print(\"Errore durante l'inizializzazione della GPU. Il modello potrebbe usare la CPU.\")\nelse:\n    print(\"Nessuna GPU rilevata. Il tuo modello verrà addestrato sulla CPU.\")\n\n# Stampa la versione di TensorFlow (utile per debugging)\nprint(f\"Versione di TensorFlow: {tf.__version__}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T16:03:06.376315Z","iopub.execute_input":"2025-07-01T16:03:06.376525Z","iopub.status.idle":"2025-07-01T16:03:07.396050Z","shell.execute_reply.started":"2025-07-01T16:03:06.376502Z","shell.execute_reply":"2025-07-01T16:03:07.395312Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Download del modello**\n\nScript per scaricare il modello generato","metadata":{}},{"cell_type":"code","source":"import os\nimport subprocess\nfrom IPython.display import FileLink, display\n\ndef download_file(path, download_file_name):\n    os.chdir('/kaggle/working/')\n    zip_name = f\"/kaggle/working/{download_file_name}.zip\"\n    command = f\"zip {zip_name} {path} -r\"\n    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n    if result.returncode != 0:\n        print(\"Unable to run zip command!\")\n        print(result.stderr)\n        return\n    display(FileLink(f'{download_file_name}.zip'))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T16:19:11.327899Z","iopub.execute_input":"2025-07-01T16:19:11.328286Z","iopub.status.idle":"2025-07-01T16:19:11.333754Z","shell.execute_reply.started":"2025-07-01T16:19:11.328263Z","shell.execute_reply":"2025-07-01T16:19:11.332952Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"download_file('/kaggle/working', 'out')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T16:39:17.247859Z","iopub.execute_input":"2025-07-01T16:39:17.248441Z","iopub.status.idle":"2025-07-01T16:39:25.275371Z","shell.execute_reply.started":"2025-07-01T16:39:17.248417Z","shell.execute_reply":"2025-07-01T16:39:25.274624Z"}},"outputs":[],"execution_count":null}]}